#!/usr/bin/env python3
"""
Generate a comprehensive list of Vietnamese vocabulary candidates.

This script combines:
1. Single words from wordfreq frequency list
2. Common compound words scored by wordfreq

Output: candidate_words.csv ranked by frequency
"""

import csv
import os
import json
from wordfreq import top_n_list, zipf_frequency
from underthesea import word_tokenize
import google.generativeai as genai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Common Vietnamese compound words by category
# These will be scored by wordfreq and merged with single words
COMPOUND_CATEGORIES = {
    'Food & Drink': [
        'th·ª©c ƒÉn', 'ƒë·ªì ƒÉn', 'c∆°m chi√™n', 'b√°nh m√¨', 'n∆∞·ªõc u·ªëng', 'ƒë·ªì u·ªëng',
        'b·ªØa s√°ng', 'b·ªØa tr∆∞a', 'b·ªØa t·ªëi', 'ƒë·ªì ng·ªçt', 'tr√°i c√¢y', 'rau c·ªß',
        'th·ªãt b√≤', 'th·ªãt g√†', 'th·ªãt l·ª£n', 'c√° bi·ªÉn', 'h·∫£i s·∫£n', 'm√≥n ƒÉn',
    ],
    'Transport': [
        'xe m√°y', 'xe ƒë·∫°p', 'xe h∆°i', 'xe √¥ t√¥', 'm√°y bay', 'xe bu√Ωt', 
        't√†u h·ªèa', 't√†u ƒëi·ªán', 'xe taxi', 'xe kh√°ch', 'ph∆∞∆°ng ti·ªán',
    ],
    'Places': [
        'b·ªánh vi·ªán', 'tr∆∞·ªùng h·ªçc', 'nh√† h√†ng', 'kh√°ch s·∫°n', 'si√™u th·ªã', 
        's√¢n bay', 'nh√† ga', 'b∆∞u ƒëi·ªán', 'ng√¢n h√†ng', 'c√¥ng vi√™n',
        'th∆∞ vi·ªán', 'b·∫£o t√†ng', 'nh√† th·ªù', 'ch√πa chi·ªÅn', 'c·ª≠a h√†ng',
        'ch·ª£ b√∫a', 'qu√°n c√† ph√™', 'ti·ªám thu·ªëc', 'ph√≤ng kh√°m',
    ],
    'People & Occupations': [
        'gi√°o vi√™n', 'h·ªçc sinh', 'sinh vi√™n', 'b√°c sƒ©', 'c√¥ng nh√¢n', 
        'nh√¢n vi√™n', 'k·ªπ s∆∞', 'lu·∫≠t s∆∞', 'ca sƒ©', 'di·ªÖn vi√™n',
        'n√¥ng d√¢n', 'th·ª£ may', 'th·ª£ ƒëi·ªán', 'th·ª£ m·ªôc', 'l√°i xe',
        'b·∫°n b√®', 'ng∆∞·ªùi y√™u', 'v·ª£ ch·ªìng', 'con c√°i', 'cha m·∫π',
        '√¥ng b√†', 'anh ch·ªã', 'em b√©', 'tr·∫ª em', 'ng∆∞·ªùi l·ªõn',
    ],
    'Work & Life': [
        'c√¥ng vi·ªác', 'cu·ªôc s·ªëng', 'gia ƒë√¨nh', 't√¨nh y√™u', 's·ª©c kh·ªèe',
        'ti·ªÅn b·∫°c', 'th·ªùi gian', 'cu·ªôc ƒë·ªùi', 't∆∞∆°ng lai', 'qu√° kh·ª©',
        'hi·ªán t·∫°i', 'th√†nh c√¥ng', 'th·∫•t b·∫°i', 'h·∫°nh ph√∫c', 'bu·ªìn b√£',
    ],
    'Technology': [
        'ƒëi·ªán tho·∫°i', 'm√°y t√≠nh', 'internet', 'website', 'email',
        'm·∫°ng x√£ h·ªôi', 'tin nh·∫Øn', 'video call', 'm√°y ·∫£nh', 'tivi',
    ],
    'Time Expressions': [
        'h√¥m nay', 'h√¥m qua', 'ng√†y mai', 'tu·∫ßn n√†y', 'nƒÉm nay',
        'th√°ng n√†y', 's√°ng nay', 't·ªëi nay', 'ƒë√™m qua', 'm·ªói ng√†y',
        'h√†ng tu·∫ßn', 'h√†ng th√°ng', 'h√†ng nƒÉm', 'l√¢u r·ªìi', 'g·∫ßn ƒë√¢y',
    ],
    'Common Expressions': [
        'nh∆∞ v·∫≠y', 'tuy nhi√™n', 'ngo√†i ra', 'v√¨ v·∫≠y', 'do ƒë√≥',
        'm·∫∑c d√π', 'd√π sao', 'c√≥ l·∫Ω', 'ch·∫Øc ch·∫Øn', 't·∫•t nhi√™n',
        'th·ª±c ra', 'th·∫≠t s·ª±', 'c√≥ th·ªÉ', 'kh√¥ng th·ªÉ', 'c·∫ßn ph·∫£i',
    ],
    'Nature & Weather': [
        'th·ªùi ti·∫øt', 'm·∫∑t tr·ªùi', 'm·∫∑t trƒÉng', 'b·∫ßu tr·ªùi', 'bi·ªÉn c·∫£',
        'n√∫i non', 's√¥ng ng√≤i', 'r·ª´ng r·∫≠m', 'ƒë·ªìng b·∫±ng', 'sa m·∫°c',
    ],
    'Body & Health': [
        'c∆° th·ªÉ', 'ƒë·∫ßu √≥c', 'tr√°i tim', 'b√†n tay', 'b√†n ch√¢n',
        'm·∫Øt m≈©i', 'tai mi·ªáng', 's·ª©c kh·ªèe', 'b·ªánh t·∫≠t', 'thu·ªëc men',
    ],
    'Education': [
        'b√†i h·ªçc', 'b√†i t·∫≠p', 'b√†i ki·ªÉm tra', 'k·ª≥ thi', 'ƒëi·ªÉm s·ªë',
        'l·ªõp h·ªçc', 'm√¥n h·ªçc', 's√°ch v·ªü', 'gi√°o d·ª•c', 'ki·∫øn th·ª©c',
    ],
    'Numbers & Measurements': [
        's·ªë l∆∞·ª£ng', 'k√≠ch th∆∞·ªõc', 'tr·ªçng l∆∞·ª£ng', 'kho·∫£ng c√°ch', 't·ªëc ƒë·ªô',
    ],
}


def setup_gemini():
    """Configure the Gemini API."""
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("‚ö†Ô∏è  GEMINI_API_KEY not found - using only predefined compounds")
        return None
    genai.configure(api_key=api_key)
    return genai.GenerativeModel('gemini-2.5-flash')


def get_additional_compounds_from_gemini(model, existing_compounds):
    """Ask Gemini to suggest additional important Vietnamese compounds."""
    if not model:
        return []
    
    print("\nü§ñ Asking Gemini for additional compound suggestions...")
    
    prompt = f"""You are a Vietnamese language expert. I need to identify the most important Vietnamese compound words (t·ª´ gh√©p) that a learner should know.

I already have these compounds: {', '.join(list(existing_compounds)[:50])}...

Please suggest 100 MORE essential Vietnamese compound words that are:
1. Commonly used in everyday speech
2. Not easily understood from individual components
3. Important for a language learner to know as a unit

Focus on categories like:
- Everyday objects and concepts
- Abstract concepts
- Common verbs/verb phrases that function as compounds
- Idiomatic expressions that are single concepts

Return ONLY a JSON array of strings, no explanation:
["compound1", "compound2", ...]"""

    try:
        response = model.generate_content(prompt)
        text = response.text.strip()
        
        # Clean up response
        if text.startswith('```'):
            text = text.split('\n', 1)[1] if '\n' in text else text[3:]
        if text.endswith('```'):
            text = text.rsplit('\n', 1)[0] if '\n' in text else text[:-3]
        text = text.strip()
        
        compounds = json.loads(text)
        print(f"  ‚úì Gemini suggested {len(compounds)} additional compounds")
        return compounds
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Gemini request failed: {e}")
        return []


def is_valid_entry(word):
    """Check if a word/compound is valid for the deck."""
    # Must have at least one Vietnamese character
    if not any('\u00C0' <= c <= '\u1EF9' or c.isalpha() for c in word):
        return False
    
    # Skip pure numbers
    if word.replace(' ', '').isdigit():
        return False
    
    # Skip single characters
    if len(word.strip()) < 2:
        return False
    
    # Skip entries that are just punctuation
    if all(c in '.,;:!?-_()[]{}' for c in word.replace(' ', '')):
        return False
    
    return True


def generate_candidate_list(output_file="candidate_words.csv"):
    """
    Generate a comprehensive list of Vietnamese vocabulary candidates.
    
    Combines single words from wordfreq with compound words, all scored by frequency.
    """
    print("="*60)
    print("Generating Vietnamese Vocabulary Candidates")
    print("="*60)
    
    # Setup Gemini
    model = setup_gemini()
    
    # Step 1: Get single words from wordfreq
    print("\nüìñ Step 1: Getting single words from wordfreq...")
    single_words = top_n_list('vi', 2500)  # Get extra to allow for filtering
    print(f"  Retrieved {len(single_words)} single words")
    
    # Step 2: Collect compound words
    print("\nüìñ Step 2: Collecting compound words...")
    compounds = set()
    for category, words in COMPOUND_CATEGORIES.items():
        for word in words:
            compounds.add(word)
    print(f"  Predefined compounds: {len(compounds)}")
    
    # Get additional compounds from Gemini
    additional = get_additional_compounds_from_gemini(model, compounds)
    for word in additional:
        if isinstance(word, str) and len(word) > 1:
            compounds.add(word.strip())
    print(f"  Total compounds after Gemini: {len(compounds)}")
    
    # Step 3: Score all entries and merge
    print("\nüìä Step 3: Scoring and ranking all entries...")
    
    all_entries = []
    seen_words = set()
    
    # Add single words with scores
    for word in single_words:
        if word in seen_words:
            continue
        if not is_valid_entry(word):
            continue
        
        freq = zipf_frequency(word, 'vi')
        if freq > 0:
            tokens = word_tokenize(word)
            all_entries.append({
                'Word': word,
                'Is_Compound': len(tokens) > 1,
                'Token_Count': len(tokens),
                'Frequency_Score': round(freq, 4)
            })
            seen_words.add(word)
    
    print(f"  Valid single words: {len(all_entries)}")
    
    # Add compound words with scores
    compounds_added = 0
    compounds_scored = 0
    for word in compounds:
        if word in seen_words:
            continue
        if not is_valid_entry(word):
            continue
        
        freq = zipf_frequency(word, 'vi')
        if freq > 0:
            compounds_scored += 1
            tokens = word_tokenize(word)
            all_entries.append({
                'Word': word,
                'Is_Compound': True,
                'Token_Count': len(tokens),
                'Frequency_Score': round(freq, 4)
            })
            seen_words.add(word)
            compounds_added += 1
    
    print(f"  Compounds with frequency scores: {compounds_added}")
    print(f"  Total candidates: {len(all_entries)}")
    
    # Step 4: Sort by frequency and rank
    print("\nüî¢ Step 4: Sorting by frequency...")
    all_entries.sort(key=lambda x: x['Frequency_Score'], reverse=True)
    
    # Add ranks
    for i, entry in enumerate(all_entries, 1):
        entry['Rank'] = i
    
    # Step 5: Write to CSV
    print(f"\nüíæ Step 5: Writing to {output_file}...")
    
    fieldnames = ['Rank', 'Word', 'Is_Compound', 'Token_Count', 'Frequency_Score']
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_entries)
    
    # Summary
    print("\n" + "="*60)
    print("‚úÖ CANDIDATE LIST GENERATED")
    print("="*60)
    print(f"üì¶ Output file: {output_file}")
    print(f"üìä Total candidates: {len(all_entries)}")
    
    # Breakdown
    single_count = sum(1 for e in all_entries if not e['Is_Compound'])
    compound_count = sum(1 for e in all_entries if e['Is_Compound'])
    print(f"\nüìà Breakdown:")
    print(f"   Single words: {single_count}")
    print(f"   Compounds: {compound_count}")
    
    # Show frequency distribution
    freq_cutoffs = [7.0, 6.5, 6.0, 5.5, 5.0, 4.5]
    print(f"\nüìä Frequency distribution:")
    for cutoff in freq_cutoffs:
        count = sum(1 for e in all_entries if e['Frequency_Score'] >= cutoff)
        print(f"   ‚â•{cutoff}: {count} entries")
    
    # Show top entries
    print(f"\nüîù Top 15 entries:")
    for entry in all_entries[:15]:
        compound_marker = "üì¶" if entry['Is_Compound'] else "  "
        print(f"   {compound_marker} #{entry['Rank']:4d} {entry['Word']:15s} (freq: {entry['Frequency_Score']:.2f})")
    
    # Show compound examples in top 500
    print(f"\nüì¶ Compounds in top 500:")
    compound_examples = [e for e in all_entries[:500] if e['Is_Compound']]
    for entry in compound_examples[:15]:
        print(f"   #{entry['Rank']:4d} {entry['Word']:15s} (freq: {entry['Frequency_Score']:.2f})")
    
    return all_entries


if __name__ == "__main__":
    generate_candidate_list(output_file="candidate_words.csv")
